Big-O Notation

constants matter
be cognizant of best-case and average-case

*f - function
an algorithm is O(f(n)) if the number of simple operations the computer has to do is eventually less than a constant f(n), as n increases

f(n) could be linear to (f(n)=n)
f(n) could be quadratic to (f(n)=n²)
f(n) could be constat to (f(n)=1)

########################################################################################

constant time - O(1) "big oh of one"

x = 5 + (15 * 20);

independent of input size, N

x = 5 + (15 * 20)
y = 15 - 2
print(x + y)

total time = O(1) + O(1) + O(1) = O(1)
				  3 * O(1)
				  
				  
########################################################################################

linear time -

N * O(1) = O(N)

for x in range(0,n):
	print(x)               <- O(1)
	
total time = O(1) + O(N) = O(N)

for loop dominates the run time, so constant time(O(1)) is dropped leaving O(N)

########################################################################################

quadratic time

O(N²)

for x in range(0, n):
	for y in range(0, n):
		print(x * y)


########################################################################################

x = 5 + (15 * 20)          <- O(1)
for x in range(0,n):       <- 
	print(x)               <- O(N)
for x in range(0,n):       <-
	for y in range(0,n):   <-
		print(x * y)       <- O(N²)
		
O(1) + O(N) + O(1) + O(N²) = O(N²)

the nested for loop dominates, O(N²)

########################################################################################

if x > 0:                  <- O(1)

else if x < 0:             <- O(logn)

else:                      <- O(n²)

longest run time is O(N²)